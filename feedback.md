# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** Rushi021
**Raw Score:** 16/50 (32.0%)
**Course Points Earned:** 96.0

---

## Problem Breakdown

### Exercise 2 (4/10 = 40.0%)

**Part ex1-part1** (ex1-part1.code): 1/4 points

_Feedback:_ You implemented PCA reconstruction, not t-SNE visualization as requested. No 2D embedding or scatter by labels using TSNE was produced. To earn full credit, compute TSNE on X_mnist_train (2 components) and scatter colored by y_mnist_train. Partial credit for DR/visualization effo

**Part ex1-part2** (ex1-part2.code): 1/3 points

_Feedback:_ You trained KNN on raw and PCA(80%) features and plotted t-SNE, but you didn’t train/evaluate KNN on the t-SNE embedding or report its accuracy. For full credit: compute t-SNE embeddings (fit on train), train KNN in t-SNE space, evaluate on test, and compare performance.

**Part ex1-part3** (ex1-part3.code): 2/3 points

_Feedback:_ You correctly trained KNN on reduced features and reported accuracy with a brief interpretation. However, this exercise required using UMAP, not t-SNE. No UMAP transform/fit/predict/accuracy shown. Use UMAP embeddings with KNN to get full credit.

---

### Exercise 4 (10/20 = 50.0%)

**Part ex2-part1** (ex2-part1.code): 2/7 points

_Feedback:_ You performed dimensionality reduction (PCA->UMAP) and a clear scatter plot, but you didn’t train KNN or report accuracy as required. The task asked to try PCA with KNN classification. Add PCA on train/test, fit KNN, predict, and print accuracy (or use your UMAP features with KNN

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You computed KNN accuracy on your 2D UMAP embedding correctly (good split/fit/predict). However, you didn’t use the provided synthetic data, didn’t compare PCA vs UMAP, didn’t vary dimensionality or UMAP parameters, and provided no visualizations for this task.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ Good work: you compared PCA (1–3D) vs UMAP (2–3D) with multiple n_neighbors and min_dist settings and summarized results. Your conclusion (PCA 3D best here) is consistent with your runs. To reach full credit, note that UMAP often excels in low dims with lower n_neighbors.

---

### Exercise 1 (2/20 = 10.0%)

**Part pipeline-part1** (pipeline-part1.code): 0/4 points

_Feedback:_ You loaded and split MNIST, but Step 2 is missing. No PCA reduction or 2D scatter plot colored by class was implemented. Add PCA().fit_transform on X_mnist_train and a plt.scatter of the first two components with c=y_mnist_train, colorbar, and title.

**Part pipeline-part2** (pipeline-part2.code): 1/4 points

_Feedback:_ You performed a 2D PCA scatter plot (Step 2), but did not compute or plot a scree plot for the first 40 components. Fit PCA with n_components>=40 on your data and plot explained_variance_ratio_[:40] (percent or proportion) vs component index.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You fit PCA(40) and plotted the scree (Step 3), but you didn’t complete Step 4. You need to compute the cumulative explained variance and find the smallest k with cumsum >= 0.95, then report k. Use np.cumsum(pca40.explained_variance_ratio_) and argmax to get the count.

**Part pipeline-part4** (pipeline-part4.code): 0/4 points

_Feedback:_ You didn’t implement Step 5. To complete: fit PCA with n_components_95, transform the chosen digit, inverse_transform it back to 784, and plot with plot_mnist_digit. As-is, only Step 4 is shown; no reduced-dim visualization provided.

**Part pipeline-part5** (pipeline-part5.code): 0/4 points

_Feedback:_ This cell doesn’t address Step 6. You only plotted a digit. You needed to train/evaluate KNN on original data and on PCA-transformed data preserving 80% variance (e.g., PCA(n_components=0.8)), then compare accuracies. Implement both and report results.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:51:59 UTC

If you have questions about your grade, please reach out to the instructor.